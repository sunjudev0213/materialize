// Copyright Materialize, Inc. and contributors. All rights reserved.
//
// Use of this software is governed by the Business Source License
// included in the LICENSE file.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0.

//! Logging dataflows for events generated by clusterd.

use std::any::Any;
use std::collections::btree_map::Entry;
use std::collections::{BTreeMap, VecDeque};
use std::rc::Rc;
use std::time::Duration;

use differential_dataflow::collection::AsCollection;
use differential_dataflow::operators::arrange::arrangement::Arrange;
use differential_dataflow::operators::arrange::Arranged;
use differential_dataflow::operators::count::CountTotal;
use differential_dataflow::trace::TraceReader;
use timely::communication::Allocate;
use timely::dataflow::operators::capture::EventLink;
use timely::dataflow::operators::generic::builder_rc::OperatorBuilder;
use timely::dataflow::operators::{Filter, InspectCore};
use timely::dataflow::{Scope, StreamCore};
use timely::logging::WorkerIdentifier;
use timely::Container;
use tracing::error;
use uuid::Uuid;

use mz_expr::{permutation_for_arrangement, MirScalarExpr};
use mz_ore::cast::CastFrom;
use mz_repr::{Datum, DatumVec, GlobalId, Row, Timestamp};
use mz_timely_util::activator::RcActivator;
use mz_timely_util::replay::MzReplay;

use crate::compute_state::ComputeState;
use crate::logging::persist::persist_sink;
use crate::logging::{ComputeLog, LogVariant};
use crate::typedefs::{KeysValsHandle, RowSpine};

/// Type alias for logging of compute events.
pub type Logger = timely::logging_core::Logger<ComputeEvent, WorkerIdentifier>;

/// A logged compute event.
#[derive(Debug, Clone, PartialOrd, PartialEq)]
pub enum ComputeEvent {
    /// A dataflow export was created.
    Export {
        /// Identifier of the export.
        id: GlobalId,
        /// Timely worker index of the exporting dataflow.
        dataflow_index: usize,
    },
    /// A dataflow export was dropped.
    ExportDropped {
        /// Identifier of the export.
        id: GlobalId,
    },
    /// Dataflow export depends on a named dataflow import.
    ExportDependency {
        /// Identifier of the export.
        export_id: GlobalId,
        /// Identifier of the import on which the export depends.
        import_id: GlobalId,
    },
    /// Peek command, true for install and false for retire.
    Peek(Peek, bool),
    /// Available frontier information for dataflow exports.
    Frontier {
        id: GlobalId,
        time: Timestamp,
        diff: i8,
    },
    /// Available frontier information for dataflow imports.
    ImportFrontier {
        import_id: GlobalId,
        export_id: GlobalId,
        time: Timestamp,
        diff: i8,
    },
}

/// A logged peek event.
#[derive(
    Debug, Clone, Ord, PartialOrd, Eq, PartialEq, Hash, serde::Serialize, serde::Deserialize,
)]
pub struct Peek {
    /// The identifier of the view the peek targets.
    id: GlobalId,
    /// The logical timestamp requested.
    time: Timestamp,
    /// The ID of the peek.
    uuid: Uuid,
}

impl Peek {
    /// Create a new peek from its arguments.
    pub fn new(id: GlobalId, time: Timestamp, uuid: Uuid) -> Self {
        Self { id, time, uuid }
    }
}

/// Constructs the logging dataflow for compute logs.
///
/// Params
/// * `worker`: The Timely worker hosting the log analysis dataflow.
/// * `config`: Logging configuration
/// * `compute`: The source to read compute log events from.
/// * `activator`: A handle to acknowledge activations.
///
/// Returns a map from log variant to a tuple of a trace handle and a permutation to reconstruct
/// the original rows.
pub fn construct<A: Allocate>(
    worker: &mut timely::worker::Worker<A>,
    config: &mz_compute_client::logging::LoggingConfig,
    compute_state: &mut ComputeState,
    compute: std::rc::Rc<EventLink<Timestamp, (Duration, WorkerIdentifier, ComputeEvent)>>,
    activator: RcActivator,
) -> BTreeMap<LogVariant, (KeysValsHandle, Rc<dyn Any>)> {
    let interval_ms = std::cmp::max(1, config.interval.as_millis());

    let traces = worker.dataflow_named("Dataflow: compute logging", move |scope| {
        let (mut compute_logs, token) =
            Some(compute).mz_replay(scope, "compute logs", config.interval, activator.clone());

        // If logging is disabled, we still need to install the indexes, but we can leave them
        // empty. We do so by immediately filtering all logs events.
        // TODO(teskje): Remove this once we remove the arranged introspection sources.
        if !config.enable_logging {
            compute_logs = compute_logs.filter(|_| false);
        }

        let mut demux = OperatorBuilder::new("Compute Logging Demux".to_string(), scope.clone());
        use timely::dataflow::channels::pact::Pipeline;
        let mut input = demux.new_input(&compute_logs, Pipeline);
        let (mut export_out, export) = demux.new_output();
        let (mut dependency_out, dependency) = demux.new_output();
        let (mut frontier_out, frontier) = demux.new_output();
        let (mut source_frontier_out, source_frontier) = demux.new_output();
        let (mut frontier_delay_out, frontier_delay) = demux.new_output();
        let (mut peek_out, peek) = demux.new_output();
        let (mut peek_duration_out, peek_duration) = demux.new_output();

        let mut demux_buffer = Vec::new();
        demux.build(move |_capability| {
            let mut export_dataflows = BTreeMap::new();
            let mut peek_stash = BTreeMap::new();

            /// State for tracking export frontier lag.
            #[derive(Default)]
            struct ImportDelayData {
                /// A list of input timestamps that have appeared on the input
                /// frontier, but that the output frontier has not yet advanced beyond,
                /// and the time at which we were informed of their availability
                time_deque: VecDeque<(mz_repr::Timestamp, u128)>,
                /// A histogram of emitted delays (bucket size to (bucket_sum, bucket_count))
                delay_map: BTreeMap<u128, (i128, i32)>,
            }
            let mut export_imports =
                BTreeMap::<(GlobalId, usize), BTreeMap<GlobalId, ImportDelayData>>::new();

            move |_frontiers| {
                let mut export = export_out.activate();
                let mut dependency = dependency_out.activate();
                let mut frontier = frontier_out.activate();
                let mut source_frontier = source_frontier_out.activate();
                let mut frontier_delay = frontier_delay_out.activate();
                let mut peek = peek_out.activate();
                let mut peek_duration = peek_duration_out.activate();

                input.for_each(|time, data| {
                    data.swap(&mut demux_buffer);

                    let mut export_session = export.session(&time);
                    let mut dependency_session = dependency.session(&time);
                    let mut frontier_session = frontier.session(&time);
                    let mut source_frontier_session = source_frontier.session(&time);
                    let mut frontier_delay_session = frontier_delay.session(&time);
                    let mut peek_session = peek.session(&time);
                    let mut peek_duration_session = peek_duration.session(&time);

                    for (time, worker, datum) in demux_buffer.drain(..) {
                        let time_ms = (((time.as_millis() / interval_ms) + 1) * interval_ms)
                            .try_into()
                            .expect("must fit");

                        match datum {
                            ComputeEvent::Export { id, dataflow_index } => {
                                export_session.give(((id, worker, dataflow_index), time_ms, 1));

                                let key = (id, worker);
                                export_dataflows.insert(key, dataflow_index);
                                export_imports.insert(key, BTreeMap::new());
                            }
                            ComputeEvent::ExportDropped { id } => {
                                let key = (id, worker);
                                if let Some(dataflow_index) = export_dataflows.remove(&key) {
                                    export_session.give((
                                        (id, worker, dataflow_index),
                                        time_ms,
                                        -1,
                                    ));
                                } else {
                                    error!(
                                        export = ?id, worker = ?worker,
                                        "missing export_dataflows entry at time of export drop"
                                    );
                                }

                                // Remove dependency and frontier delay logging for this export.
                                if let Some(imports) = export_imports.remove(&key) {
                                    for (import_id, delay) in imports {
                                        dependency_session.give((
                                            (id, import_id, worker),
                                            time_ms,
                                            -1,
                                        ));

                                        for (delay_pow, (delay_sum, delay_count)) in delay.delay_map
                                        {
                                            frontier_delay_session.give((
                                                (
                                                    id,
                                                    import_id,
                                                    worker,
                                                    delay_pow,
                                                    delay_sum,
                                                    delay_count,
                                                ),
                                                time_ms,
                                                -1,
                                            ));
                                        }
                                    }
                                } else {
                                    error!(
                                        export = ?id, worker = ?worker,
                                        "missing export_imports entry at time of export drop"
                                    );
                                }
                            }
                            ComputeEvent::ExportDependency {
                                export_id,
                                import_id,
                            } => {
                                dependency_session.give((
                                    (export_id, import_id, worker),
                                    time_ms,
                                    1,
                                ));

                                let key = (export_id, worker);
                                if let Some(imports) = export_imports.get_mut(&key) {
                                    imports.insert(import_id, Default::default());
                                } else {
                                    error!(
                                        export = ?export_id, import = ?import_id, worker = ?worker,
                                        "tried to create import for export that doesn't exist"
                                    );
                                }
                            }
                            ComputeEvent::Frontier {
                                id,
                                time: logical,
                                diff,
                            } => {
                                // report dataflow frontier advancement
                                frontier_session.give((
                                    Row::pack_slice(&[
                                        Datum::String(&id.to_string()),
                                        Datum::UInt64(u64::cast_from(worker)),
                                        Datum::MzTimestamp(logical),
                                    ]),
                                    time_ms,
                                    i64::from(diff),
                                ));
                                if diff > 0 {
                                    // Check if we have imports associated to this export
                                    // and report frontier advancement delays.
                                    let key = (id, worker);
                                    if let Some(import_map) = export_imports.get_mut(&key) {
                                        for (
                                            import_id,
                                            ImportDelayData {
                                                time_deque,
                                                delay_map,
                                            },
                                        ) in import_map
                                        {
                                            while let Some(current_front) = time_deque.pop_front() {
                                                let import_logical = current_front.0;
                                                if logical >= import_logical {
                                                    let elapsed_ns =
                                                        time.as_nanos() - current_front.1;
                                                    let elapsed_pow =
                                                        elapsed_ns.next_power_of_two();
                                                    let elapsed_ns: i128 = elapsed_ns
                                                        .try_into()
                                                        .expect("elapsed_ns too big");
                                                    let (new_delay_sum, new_delay_count) =
                                                        match delay_map.entry(elapsed_pow) {
                                                            Entry::Vacant(v) => v.insert((0, 0)),
                                                            Entry::Occupied(o) => {
                                                                let (
                                                                    old_delay_sum,
                                                                    old_delay_count,
                                                                ) = o.get().clone();
                                                                frontier_delay_session.give((
                                                                    (
                                                                        id,
                                                                        *import_id,
                                                                        worker,
                                                                        elapsed_pow,
                                                                        old_delay_sum,
                                                                        old_delay_count,
                                                                    ),
                                                                    time_ms,
                                                                    -1,
                                                                ));
                                                                o.into_mut()
                                                            }
                                                        };
                                                    *new_delay_sum += elapsed_ns;
                                                    *new_delay_count += 1;
                                                    frontier_delay_session.give((
                                                        (
                                                            id,
                                                            *import_id,
                                                            worker,
                                                            elapsed_pow,
                                                            *new_delay_sum,
                                                            *new_delay_count,
                                                        ),
                                                        time_ms,
                                                        1,
                                                    ));
                                                } else {
                                                    time_deque.push_front(current_front);
                                                    break;
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                            ComputeEvent::ImportFrontier {
                                import_id,
                                export_id,
                                time: logical,
                                diff,
                            } => {
                                // report import frontier advancement
                                source_frontier_session.give((
                                    Row::pack_slice(&[
                                        Datum::String(&export_id.to_string()),
                                        Datum::String(&import_id.to_string()),
                                        Datum::UInt64(u64::cast_from(worker)),
                                        Datum::MzTimestamp(logical),
                                    ]),
                                    time_ms,
                                    i64::from(diff),
                                ));
                                if diff > 0 {
                                    // Note that it is possible that we receive frontier updates
                                    // for exports no longer present in `export_imports`. This
                                    // behavior arises because `ImportFrontier` events are
                                    // generated by a dataflow `inspect_container` operator, which
                                    // may outlive the corresponding trace or sink recording in the
                                    // current `ComputeState` until Timely eventually drops it.
                                    let key = (export_id, worker);
                                    if let Some(import_map) = export_imports.get_mut(&key) {
                                        if let Some(time_entry) = import_map.get_mut(&import_id) {
                                            time_entry.time_deque.push_back((logical, time.as_nanos()));
                                        } else {
                                            error!(
                                                export = ?export_id, import = ?import_id, worker = ?worker,
                                                "tried to create update frontier for import that doesn't exist"
                                            );
                                        }
                                    }
                                }
                            }
                            ComputeEvent::Peek(peek, is_install) => {
                                let key = (worker, peek.uuid);
                                if is_install {
                                    peek_session.give(((peek, worker), time_ms, 1));
                                    if peek_stash.contains_key(&key) {
                                        error!(
                                            "peek already registered: \
                                             worker={}, uuid: {}",
                                            worker, key.1,
                                        );
                                    }
                                    peek_stash.insert(key, time.as_nanos());
                                } else {
                                    peek_session.give(((peek, worker), time_ms, -1));
                                    if let Some(start) = peek_stash.remove(&key) {
                                        let elapsed_ns = time.as_nanos() - start;
                                        let elapsed_pow = elapsed_ns.next_power_of_two();
                                        let elapsed_ns: i128 =
                                            elapsed_ns.try_into().expect("elapsed_ns too big");
                                        peek_duration_session.give((
                                            (key.0, elapsed_pow),
                                            time_ms,
                                            (elapsed_ns, 1u64),
                                        ));
                                    } else {
                                        error!(
                                            "peek not yet registered: \
                                             worker={}, uuid: {}",
                                            worker, key.1,
                                        );
                                    }
                                }
                            }
                        }
                    }
                });
            }
        });

        let dataflow_current = export.as_collection().map({
            move |(name, worker, dataflow_id)| {
                Row::pack_slice(&[
                    Datum::String(&name.to_string()),
                    Datum::UInt64(u64::cast_from(worker)),
                    Datum::UInt64(u64::cast_from(dataflow_id)),
                ])
            }
        });

        let dependency_current = dependency.as_collection().map({
            move |(dataflow, source, worker)| {
                Row::pack_slice(&[
                    Datum::String(&dataflow.to_string()),
                    Datum::String(&source.to_string()),
                    Datum::UInt64(u64::cast_from(worker)),
                ])
            }
        });

        let frontier_current = frontier.as_collection();

        let source_frontier_current = source_frontier.as_collection();

        let frontier_delay = frontier_delay.as_collection().map({
            move |(dataflow, source_id, worker, delay_pow, delay_sum, delay_count)| {
                Row::pack_slice(&[
                    Datum::String(&dataflow.to_string()),
                    Datum::String(&source_id.to_string()),
                    Datum::UInt64(u64::cast_from(worker)),
                    Datum::UInt64(delay_pow.try_into().expect("pow too big")),
                    Datum::Int64(delay_count.into()),
                    // [btv] This is nullable so that we can fill
                    // in `NULL` if it overflows. That would be a
                    // bit far-fetched, but not impossible to
                    // imagine. See discussion
                    // [here](https://github.com/MaterializeInc/materialize/pull/17302#discussion_r1086373740)
                    // for more details, and think about this
                    // again if we ever decide to stabilize it.
                    u64::try_from(delay_sum).ok().into(),
                ])
            }
        });

        let peek_current = peek.as_collection().map({
            move |(peek, worker)| {
                Row::pack_slice(&[
                    Datum::Uuid(peek.uuid),
                    Datum::UInt64(u64::cast_from(worker)),
                    Datum::String(&peek.id.to_string()),
                    Datum::MzTimestamp(peek.time),
                ])
            }
        });

        // Duration statistics derive from the non-rounded event times.
        let peek_duration = peek_duration
            .as_collection()
            .arrange_named::<RowSpine<_, _, _, _>>("Arranged timely peek_duration")
            .count_total_core()
            .map(|((worker, bucket), (sum, count))| {
                Row::pack_slice(&[
                    Datum::UInt64(u64::cast_from(worker)),
                    Datum::UInt64(bucket.try_into().expect("pow too big")),
                    Datum::UInt64(count),
                    // [btv] This is nullable so that we can fill
                    // in `NULL` if it overflows. That would be a
                    // bit far-fetched, but not impossible to
                    // imagine. See discussion
                    // [here](https://github.com/MaterializeInc/materialize/pull/17302#discussion_r1086373740)
                    // for more details, and think about this
                    // again if we ever decide to stabilize it.
                    u64::try_from(sum).ok().into(),
                ])
            });

        let logs = vec![
            (
                LogVariant::Compute(ComputeLog::DataflowCurrent),
                dataflow_current,
            ),
            (
                LogVariant::Compute(ComputeLog::DataflowDependency),
                dependency_current,
            ),
            (
                LogVariant::Compute(ComputeLog::FrontierCurrent),
                frontier_current,
            ),
            (
                LogVariant::Compute(ComputeLog::ImportFrontierCurrent),
                source_frontier_current,
            ),
            (
                LogVariant::Compute(ComputeLog::FrontierDelay),
                frontier_delay,
            ),
            (LogVariant::Compute(ComputeLog::PeekCurrent), peek_current),
            (LogVariant::Compute(ComputeLog::PeekDuration), peek_duration),
        ];

        let mut result = BTreeMap::new();
        for (variant, collection) in logs {
            if config.index_logs.contains_key(&variant) {
                let key = variant.index_by();
                let (_, value) = permutation_for_arrangement(
                    &key.iter()
                        .cloned()
                        .map(MirScalarExpr::Column)
                        .collect::<Vec<_>>(),
                    variant.desc().arity(),
                );
                let trace = collection
                    .map({
                        let mut row_buf = Row::default();
                        let mut datums = DatumVec::new();
                        move |row| {
                            let datums = datums.borrow_with(&row);
                            row_buf.packer().extend(key.iter().map(|k| datums[*k]));
                            let row_key = row_buf.clone();
                            row_buf.packer().extend(value.iter().map(|k| datums[*k]));
                            let row_val = row_buf.clone();
                            (row_key, row_val)
                        }
                    })
                    .arrange_named::<RowSpine<_, _, _, _>>(&format!("ArrangeByKey {:?}", variant))
                    .trace;
                result.insert(variant.clone(), (trace, Rc::clone(&token)));
            }

            if let Some((id, meta)) = config.sink_logs.get(&variant) {
                tracing::debug!("Persisting {:?} to {:?}", &variant, meta);
                persist_sink(*id, meta, compute_state, collection);
            }
        }
        result
    });

    traces
}

pub(crate) trait LogImportFrontiers {
    fn log_import_frontiers(
        self,
        logger: Logger,
        import_id: GlobalId,
        export_ids: Vec<GlobalId>,
    ) -> Self;
}

impl<G, C> LogImportFrontiers for StreamCore<G, C>
where
    G: Scope<Timestamp = Timestamp>,
    C: Container,
{
    fn log_import_frontiers(
        self,
        logger: Logger,
        import_id: GlobalId,
        export_ids: Vec<GlobalId>,
    ) -> Self {
        let mut previous_time = None;
        self.inspect_container(move |event| {
            if let Err(frontier) = event {
                if let Some(previous) = previous_time {
                    for &export_id in export_ids.iter() {
                        logger.log(ComputeEvent::ImportFrontier {
                            import_id,
                            export_id,
                            time: previous,
                            diff: -1,
                        });
                    }
                }
                if let Some(time) = frontier.get(0) {
                    for &export_id in export_ids.iter() {
                        logger.log(ComputeEvent::ImportFrontier {
                            import_id,
                            export_id,
                            time: *time,
                            diff: 1,
                        });
                    }
                    previous_time = Some(*time);
                } else {
                    previous_time = None;
                }
            }
        })
    }
}

impl<G, Tr> LogImportFrontiers for Arranged<G, Tr>
where
    G: Scope<Timestamp = Timestamp>,
    Tr: TraceReader + Clone,
{
    fn log_import_frontiers(
        mut self,
        logger: Logger,
        import_id: GlobalId,
        export_ids: Vec<GlobalId>,
    ) -> Self {
        self.stream = self
            .stream
            .log_import_frontiers(logger, import_id, export_ids);
        self
    }
}
