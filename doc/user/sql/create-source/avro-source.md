---
title: "CREATE SOURCE: Avro over Kafka"
description: "Learn how to connect Materialize to an Avro-formatted Kafka topic"
menu:
  main:
    parent: 'create-source'
aliases:
    - /docs/sql/create-source/kafka
    - /docs/sql/create-source/avro
---

`CREATE SOURCE` connects Materialize to some data source, and lets you interact
with its data as if it were in a SQL table.

This document details how to connect Materialize to an Avro-formatted Kafka
topic. For other options, view [`CREATE  SOURCE`](../).

## Conceptual framework

Sources represent connections to resources outside Materialize that it can read
data from. For more information, see [API Components:
Sources](../../../overview/api-components#sources).

## Syntax

{{< diagram "create-source-avro.html" >}}

Field | Use
------|-----
_src&lowbar;name_ | The name for the source, which is used as its table name within SQL.
_col&lowbar;name_ | Override default column name with the provided [identifier](../../identifiers). If used, a _col&lowbar;name_ must be provided for each column in the created source.
**KAFKA BROKER** _host_ | The Kafka broker's host name.
**FORMAT ...** _url_ | The URL of the Confluent schema registry to get schema information from.
**ENVELOPE** _envelope_ | The envelope type.<br/><br/> &#8226; **NONE** creates an append-only source. This means that records will only be appended and cannot be updated or deleted. <br/><br/>&#8226; **DEBEZIUM** creates a source that can reflect all CRUD operations from the source. This option requires records have the [appropriate fields](#format-implications), and is generally only supported by sources published to Kafka by [Debezium]. For more information, see [Debezium envelope details](#debezium-envelope-details).<br/><br/>&#8226; **UPSERT** creates a source that treats later records with the same key as an earlier record as updates to the earlier record. For more information see [Upsert envelope details](#upsert-envelope-details)

### Format specification

{{< diagram "upsert-format-spec.html" >}}

Field | Use
------|-----
**CONFLUENT SCHEMA REGISTRY ...** _url_ | The URL of the Confluent schema registry to get schema information from.
**SCHEMA FILE ...** _schema&lowbar;file&lowbar;path_ | The absolute path to a file containing the schema.
**SCHEMA ...** _inline&lowbar;schema_ | A string representing the schema.

## Details

This document assumes you're using Kafka to send Avro-encoded data, raw
bytes, or plain text to Materialize.

### Kafka source details

Materialize expects each source to use one Kafka topic, which is&mdash;in
  turn&mdash;generated by a single table in an upstream database.

### Avro format details

Avro-formatted external sources require providing you providing the schema in
one of three ways:

- Using the [Confluent Schema Registry](#using-a-confluent-schema-registry)
- Providing a path to a file that contains the Avro Schema.
- Providing the Avro schema [in-line when creating the
  source](#inlining-the-avro-schema).

### Text format details

Text-formatted data:
- is assumed to be UTF-8 encoded, and discarded if it cannot be converted
  to UTF-8.
- is treated as having one column, which, by default, is named `text`.

### Raw byte format details

No formatting or decoding is applied to raw byte-formatted data.

Raw byte-formatted sources is treated as having one column, which, by default,
is named `bytes`.

### Envelope details

Envelopes determine whether an incoming record inserts new data, updates or
deletes existing data, or both. For more information, see [API Components:
Envelopes](../../../overview/api-components#envelopes).

Avro-encoded data sent through Kafka is the only type of data that can support
the Debezium envelope.

### Debezium envelope details

The Debezium envelope provides a "diff envelope", which describes the decoded
records' old and new values; this is roughly equivalent to the notion of Change
Data Capture, or CDC. Materialize can use the data in this diff envelope to
process data as representing inserts, updates, or deletes.

This envelope is called the Debezium envelope because it's been developed to
explicitly work with [Debezium].

To use the Debezium envelope with Materialize, you must configure Debezium with
your database.

- [MySQL](https://debezium.io/documentation/reference/0.10/connectors/mysql.html)
- [PostgreSQL](https://debezium.io/documentation/reference/0.10/connectors/postgresql.html)

The Debezium envelope is only supported by sources published to Kafka by
Debezium.

#### Format implications

Using the Debezium envelopes changes the schema of your Avro-encoded Kafka
topics to include something akin to the following field:

```json
{
    "type": "record",
    "name": "envelope",
    "fields": [
        {
        "name": "before",
        "type": [
            {
            "name": "row",
            "type": "record",
            "fields": [
                {"name": "a", "type": "long"},
                {"name": "b", "type": "long"}
            ]
            },
            "null"
        ]
        },
        { "name": "after", "type": ["row", "null"] }
    ]
}
```

Note that:

- If you use the Confluent Schema Registry to receive your schemas, you don't
  need to manually create this field; Debezium will have taken care of it for
  you.
- The following section depends on the column's names and types, and is unlikely
  to match our example:
    ```json
    ...
    "fields": [
            {"name": "a", "type": "long"},
            {"name": "b", "type": "long"}
        ]
    ...
    ```
### Upsert envelope details

Specifying `ENVELOPE UPSERT` creates a source that supports Kafka's standard
key-value convention. The source is also compatible with Kafka's log-compaction
feature.

If there is a record with a later offset with the same key as a record with an
earlier offset, Materialize will delete the record with the earlier offset and
replace it with the record with the later offset. If the record with the later
offset has a null payload, Materialize will delete the record with the earlier
offset and not replace it.

As with any other source, you can interact with upsert sources as if it were an
SQL table. But unlike other sources, you can interact with the message key
columns in addition to the message payload columns. By default, the message key
columns come before the message payload columns.

As the format of the message key may be different from the format of the message
payload, when specifying the upsert envelope, you can add an optional format
specification for the message key. 

If you do not include the format specification, Materialize will treat the
message key as having the same format as the message payload. The exception if
you are using the Confluent Schema Registry to receive your schemas. Materialize
will look for the key and payload schemas based on TopicNameStrategy.

If the format of the key is either plain text or raw bytes, the key is treated
as single column. The default key column name is `key0`. 

## Examples

### Using a Confluent schema registry

```sql
CREATE SOURCE events
FROM KAFKA BROKER 'localhost:9092' TOPIC 'events'
FORMAT AVRO
    USING CONFLUENT SCHEMA REGISTRY 'http://localhost:8081';
```

This creates a source that...

- Automatically determines its schema from the Confluent Schema Registry.
- Decodes data received from the `events` topic published by Kafka running on
  `localhost:9092`.
- Decodes using an Avro schema.
- Is eligible to use the Debezium envelope because it's Avro-encoded and
  published by Kafka; however, this still depends on whether or not the upstream
  database publishes its data from a Debezium-enabled database.

### Inlining the Avro schema

```sql
CREATE SOURCE user
FROM KAFKA BROKER 'localhost:9092' TOPIC 'user'
FORMAT AVRO
USING SCHEMA '{
  "type": "record",
  "name": "envelope",
  "fields": [
    ...
  ],
}'
    ENVELOPE DEBEZIUM;
```

This creates a source that...

- Has its schema defined inline, and decodes data using that schema.
- Decodes data received from the `user` topic published by Kafka running on
  `localhost:9092`.
- Uses the Debezium envelope, meaning it supports delete, updates, and inserts.

### Upsert on a Kafka topic with string keys and Avro values

```sql
CREATE SOURCE current_predictions
FROM KAFKA BROKER 'localhost:9092' TOPIC 'current_predictions'
FORMAT AVRO
USING SCHEMA FILE '/scratch/current_predictions.json'
ENVELOPE UPSERT FORMAT TEXT;
```

This creates a source that...

- Has its schema in a file on disk, and decodes payload data using that schema.
- Decodes data received from the `user` topic published by Kafka running on
  `localhost:9092`.
- Uses message keys to determine what should be inserted, deleted, and updated.
- Treats message keys as plain text.

## Related pages

- [`CREATE SOURCE`](../)
- [`CREATE VIEW`](../../create-view)
- [`SELECT`](../../select)

[Debezium]: http://debezium.io
